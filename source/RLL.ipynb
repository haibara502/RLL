{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferWeight(data, alpha=None, beta=None):    \n",
    "    votes = data[:,1]\n",
    "    maxVote = max(votes)\n",
    "    weights = []\n",
    "    for i in range(votes.shape[0]):\n",
    "        v = votes[i]\n",
    "        if(v>=(1+maxVote)/2):\n",
    "            if(alpha==None or beta==None):\n",
    "                weights.append(float(v/maxVote))\n",
    "            else:\n",
    "                weights.append(float((v+alpha)/(maxVote+alpha+beta)))\n",
    "        else:\n",
    "            if(alpha==None or beta==None):\n",
    "                weights.append(1-float(v/maxVote))\n",
    "            else:\n",
    "                weights.append(float((maxVote-v+alpha)/(maxVote+alpha+beta)))\n",
    "    data[:,1] = weights\n",
    "    return data\n",
    "\n",
    "def splitFeatureWeight(x):\n",
    "    return x[:,1:], x[:,0]\n",
    "\n",
    "def createGroupsRandom(data, groupSize=int(4e5)):\n",
    "    positive = data[np.where(data[:,0]==1)]\n",
    "    negative = data[np.where(data[:,0]==0)]\n",
    "    posNum = positive.shape[0]\n",
    "    negNum = negative.shape[0]\n",
    "    idx = np.random.randint(low=0, high=posNum, size=groupSize)\n",
    "    query = np.array([positive[i,1:] for i in idx])\n",
    "    posDoc = shuffle(query)\n",
    "    idx = np.random.randint(low=0, high=negNum, size=groupSize)\n",
    "    negDoc0 = np.array([negative[i,1:] for i in idx])\n",
    "    negDoc1 = shuffle(negDoc0)\n",
    "    negDoc2 = shuffle(negDoc0)\n",
    "    \n",
    "    query, _ = splitFeatureWeight(query)\n",
    "    posDoc, posDocW = splitFeatureWeight(posDoc)\n",
    "    negDoc0, negDoc0W = splitFeatureWeight(negDoc0)\n",
    "    negDoc1, negDoc1W = splitFeatureWeight(negDoc1)\n",
    "    negDoc2, negDoc2W = splitFeatureWeight(negDoc2)\n",
    "    \n",
    "    groups = (query, posDoc, negDoc0, negDoc1, negDoc2)\n",
    "    weights = (posDocW, negDoc0W, negDoc1W, negDoc2W)\n",
    "    return groups, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLL framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code implements the RLL framework and its variants\n",
    "paper published on \"Learning Effective Embeddings From Crowdsourced Labels: An Educational Case Study\", ICDE 2019\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters\n",
    "\n",
    "bs: batch size\n",
    "lr_rate: learning rate\n",
    "l1_n: number of neurons in the first layer\n",
    "l2_n: number of neurons in the second layer\n",
    "max_iter: max interation number for training\n",
    "reg_scale: regularization penalty\n",
    "dropout_rate: ratio to drop neurons in each layer\n",
    "gamma: a held-out hyperparameter in loss function, we set it to 10.0 in our experiments\n",
    "weighted: True or False. If true, weights(label confidence based on votes of an example) are integrated into loss function\n",
    "save_path: where you save the model\n",
    "model_name: you can name your model however you like\n",
    "'''\n",
    "\n",
    "\n",
    "def helper(df, groupSize=int(4e5)):\n",
    "    tmp= [df.columns[1]] + [df.columns[0]] +list(df.columns[2:])\n",
    "    df = df[tmp]\n",
    "    data = np.array(df)\n",
    "    data = inferWeight(data)\n",
    "    groups, weights = createGroupsRandom(data, groupSize)\n",
    "    return groups, weights\n",
    "\n",
    "\n",
    "#compute cosine distance between two vectors\n",
    "def cos_sim(a, b):\n",
    "    normalize_a = tf.nn.l2_normalize(a,1)        \n",
    "    normalize_b = tf.nn.l2_normalize(b,1) \n",
    "    cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b), axis=1)\n",
    "    return cos_similarity\n",
    "    \n",
    "class RLL(object):\n",
    "    def __init__(self, dimension, l1_n, l2_n, gamma):\n",
    "        self.dimension = dimension\n",
    "        self.l1_n = l1_n\n",
    "        self.l2_n = l2_n\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def feedBatch(self, groups, weights, batchSize, is_training):\n",
    "        batchIndex = np.random.randint(low=0, high=groups[0].shape[0], size=batchSize)\n",
    "        batchGroups = [groups[i][batchIndex] for i in range(len(groups))]\n",
    "        batchWeights = [weights[i][batchIndex] for i in range(len(weights))]\n",
    "        batchData = {\n",
    "                            self.is_training: is_training,\n",
    "                            self.query: batchGroups[0], \n",
    "                            self.posDoc : batchGroups[1], \n",
    "                            self.negDoc0 :batchGroups[2], \n",
    "                            self.negDoc1 : batchGroups[3], \n",
    "                            self.negDoc2: batchGroups[4],\n",
    "                            self.posDocW: batchWeights[0].reshape(-1, ),    \n",
    "                            self.negDoc0W: batchWeights[1].reshape(-1, ),\n",
    "                            self.negDoc1W: batchWeights[2].reshape(-1, ),\n",
    "                            self.negDoc2W: batchWeights[3].reshape(-1,) \n",
    "            }\n",
    "        return batchData\n",
    "    \n",
    "    \n",
    "    def buildRLL(self, lr_rate, max_iter, reg_scale, dropout_rate):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.is_training = tf.placeholder_with_default(False, shape=(), name='isTraining')\n",
    "        self.query = tf.placeholder(tf.float32, shape=[None, self.dimension], name='queryInput')\n",
    "        self.posDoc = tf.placeholder(tf.float32, shape=[None, self.dimension], name='posDocInput')\n",
    "        self.negDoc0 = tf.placeholder(tf.float32, shape=[None, self.dimension], name='negDoc0Input')\n",
    "        self.negDoc1 = tf.placeholder(tf.float32, shape=[None, self.dimension], name='negDoc1Input')\n",
    "        self.negDoc2 = tf.placeholder(tf.float32, shape=[None, self.dimension], name='negDoc2Input')\n",
    "        self.posDocW = tf.placeholder(tf.float32, shape=[None], name='posDocWeight')\n",
    "        self.negDoc0W = tf.placeholder(tf.float32, shape=[None], name='negDoc0Weight')\n",
    "        self.negDoc1W = tf.placeholder(tf.float32, shape=[None], name='negDoc1Weight')\n",
    "        self.negDoc2W = tf.placeholder(tf.float32, shape=[None], name='negDoc2Weight')\n",
    "\n",
    "        with tf.name_scope('fc_l1_query'):\n",
    "            outputQuery = tf.contrib.layers.fully_connected(self.query, self.l1_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                 activation_fn = tf.nn.sigmoid, scope='fc_l1_query')\n",
    "            outputQuery = tf.layers.dropout(outputQuery, dropout_rate, training=self.is_training)\n",
    "        with tf.name_scope('fc_l1_doc'):\n",
    "            outputPosDoc =  tf.contrib.layers.fully_connected(self.posDoc, self.l1_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                activation_fn = tf.nn.sigmoid, scope='fc_l1_doc')\n",
    "            outputPosDoc = tf.layers.dropout(outputPosDoc, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc0 =  tf.contrib.layers.fully_connected(self.negDoc0, self.l1_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                 activation_fn = tf.nn.sigmoid, scope='fc_l1_doc', reuse=True)\n",
    "            outputNegDoc0=tf.layers.dropout(outputNegDoc0, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc1 =  tf.contrib.layers.fully_connected(self.negDoc1, self.l1_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                  activation_fn = tf.nn.sigmoid, scope='fc_l1_doc', reuse=True)\n",
    "            outputNegDoc1 = tf.layers.dropout(outputNegDoc1, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc2 =  tf.contrib.layers.fully_connected(self.negDoc2, self.l1_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                  activation_fn = tf.nn.sigmoid, scope='fc_l1_doc', reuse=True)\n",
    "\n",
    "            outputNegDoc2=tf.layers.dropout(outputNegDoc2, dropout_rate, training=self.is_training)\n",
    "\n",
    "\n",
    "        with tf.name_scope('fc_l2_query'):\n",
    "            outputQuery = tf.contrib.layers.fully_connected(outputQuery, self.l2_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                             activation_fn = tf.nn.sigmoid, scope='fc_l2_query')\n",
    "            outputQuery = tf.layers.dropout(outputQuery, dropout_rate, training=self.is_training)\n",
    "        with tf.name_scope('fc_l2_doc'):\n",
    "            outputPosDoc = tf.contrib.layers.fully_connected(outputPosDoc, self.l2_n, weights_regularizer = layers.l2_regularizer(reg_scale), \n",
    "                                                               activation_fn = tf.nn.sigmoid, scope='fc_l2_doc')\n",
    "            outputPosDoc = tf.layers.dropout(outputPosDoc, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc0 = tf.contrib.layers.fully_connected(outputNegDoc0, self.l2_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                activation_fn = tf.nn.sigmoid, scope='fc_l2_doc', reuse=True)\n",
    "            outputNegDoc0 = tf.layers.dropout(outputNegDoc0, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc1 = tf.contrib.layers.fully_connected(outputNegDoc1, self.l2_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                activation_fn = tf.nn.sigmoid, scope='fc_l2_doc', reuse=True)\n",
    "            outputNegDoc1 = tf.layers.dropout(outputNegDoc1, dropout_rate, training=self.is_training)\n",
    "\n",
    "            outputNegDoc2 = tf.contrib.layers.fully_connected(outputNegDoc2, self.l2_n, weights_regularizer = layers.l2_regularizer(reg_scale),\\\n",
    "                                                                activation_fn = tf.nn.sigmoid, scope='fc_l2_doc', reuse=True)\n",
    "\n",
    "            outputNegDoc2 = tf.layers.dropout(outputNegDoc2, dropout_rate, training=self.is_training)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            reg_ws_0 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, 'fc_l1_query')\n",
    "            reg_ws_1 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, 'fc_l1_doc')\n",
    "            reg_ws_2 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, 'fc_l2_query')\n",
    "            reg_ws_3 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, 'fc_l2_doc')\n",
    "            reg_loss = tf.reduce_sum(reg_ws_0)+tf.reduce_sum(reg_ws_1)+tf.reduce_sum(reg_ws_2)+tf.reduce_sum(reg_ws_3)\n",
    "\n",
    "            nominator = tf.multiply(self.posDocW, tf.exp(tf.multiply(self.gamma, cos_sim(outputQuery, outputPosDoc))))\n",
    "            doc0_similarity = tf.multiply(self.negDoc0W, tf.exp(tf.multiply(self.gamma, cos_sim(outputQuery, outputNegDoc0))))\n",
    "            doc1_similarity = tf.multiply(self.negDoc1W, tf.exp(tf.multiply(self.gamma, cos_sim(outputQuery, outputNegDoc1))))\n",
    "            doc2_similarity = tf.multiply(self.negDoc2W, tf.exp(tf.multiply(self.gamma, cos_sim(outputQuery, outputNegDoc2))))\n",
    "            prob = tf.add(nominator,tf.constant(1e-10))/tf.add(doc0_similarity+ doc1_similarity+doc2_similarity+nominator,tf.constant(1e-10))\n",
    "            log_prob = tf.log(prob)\n",
    "            self.loss = -tf.reduce_sum(log_prob) + reg_loss\n",
    "\n",
    "        with tf.name_scope('Optimizer'):\n",
    "            self.optimizer = tf.train.AdadeltaOptimizer(lr_rate).minimize(self.loss)\n",
    "        \n",
    "    def train(self, groupsTr, weightsTr, groupsVal, weightsVal, batchSize):\n",
    "        train_size = groupsTr[0].shape[0]\n",
    "        print('training group size is {}'.format(train_size))\n",
    "        val_size = groupsVal[0].shape[0]\n",
    "        print('validation group size is {}'.format(val_size))\n",
    "\n",
    "        best_val_loss = sys.maxsize\n",
    "        num_batch = train_size//batchSize\n",
    "        earlyStopCount = 0\n",
    "        saver = tf.train.Saver(max_to_keep=1)\n",
    "        model_name = 'RLL_l1_{}_l2_{}_lr_{}_penalty_{}_bs_{}'.format(self.l1_n, self.l2_n, lr_rate, reg_scale, batchSize)\n",
    "        \n",
    "        currentModelPath = os.path.join(save_path, model_name)\n",
    "        if(not os.path.exists(currentModelPath)):\n",
    "            os.makedirs(currentModelPath)\n",
    "        \n",
    "        start = time.time()\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(0, max_iter):\n",
    "                total_loss = 0\n",
    "                for batch in range(num_batch):\n",
    "                    batchData = self.feedBatch(groupsTr, weightsTr, batchSize, is_training=True)\n",
    "                    _, batch_loss = sess.run([self.optimizer, self.loss], feed_dict=batchData)\n",
    "                    total_loss += batch_loss\n",
    "                print(\"Epoch {} train loss {}\".format(epoch, total_loss/train_size))\n",
    "                if(epoch%10==0):\n",
    "                    valData = self.feedBatch(groupsVal, weightsVal, groupsVal[0].shape[0], is_training=False)\n",
    "                    valLoss = sess.run(self.loss, feed_dict=valData)\n",
    "                    print('*'*66)\n",
    "                    print(\"Epoch {} validation loss {}\".format(epoch, valLoss/val_size))\n",
    "                    print('\\n')\n",
    "                    if(valLoss<best_val_loss):\n",
    "                        best_val_loss = valLoss\n",
    "                        earlyStopCount = 0\n",
    "                        saver.save(sess, os.path.join(currentModelPath, model_name+'.ckpt'))\n",
    "                    else:\n",
    "                        earlyStopCount += 1\n",
    "                if(earlyStopCount>=5):\n",
    "                    print('Early stop at epoch {}!'.format(epoch))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../raw_data/train.csv').drop(columns=['id'])\n",
    "validation = pd.read_csv('../raw_data/validation.csv').drop(columns=['id'])\n",
    "\n",
    "groupsTr, weightsTr = helper(train)\n",
    "groupsVal, weightsVal = helper(validation, groupSize=int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training group size is 400000\n",
      "validation group size is 100000\n",
      "Epoch 0 train loss 1.3145025410461426\n",
      "******************************************************************\n",
      "Epoch 0 validation loss 0.863957578125\n",
      "\n",
      "\n",
      "Epoch 1 train loss 1.024658652496338\n",
      "Epoch 2 train loss 0.9359941521453857\n",
      "Epoch 3 train loss 0.8881271532440186\n",
      "Epoch 4 train loss 0.8569887702941894\n",
      "Epoch 5 train loss 0.8280903089141846\n",
      "Epoch 6 train loss 0.8034984691619873\n",
      "Epoch 7 train loss 0.7827501516723633\n",
      "Epoch 8 train loss 0.7590772065734863\n"
     ]
    }
   ],
   "source": [
    "dropout_rate = 0.5\n",
    "dimension = 50\n",
    "gamma = 10.0\n",
    "max_iter = 888\n",
    "lr_rate = 0.05\n",
    "\n",
    "l1_n_lst = [256, 128, 64]\n",
    "l2_n_lst = [128, 64, 32]\n",
    "reg_scale_lst = [0.1, 1, 5, 10]\n",
    "batchSize_lst = [1024, 512, 256, 128]\n",
    "\n",
    "for l1_n in l1_n_lst:\n",
    "    for l2_n in l2_n_lst:\n",
    "        for reg_scale in reg_scale_lst:\n",
    "            for batchSize in batchSize_lst:\n",
    "                try:\n",
    "                    model = RLL(dimension, l1_n, l2_n, gamma)\n",
    "                    save_path = '/workspace/Guowei/rll/model'\n",
    "                    model.buildRLL(lr_rate, max_iter, reg_scale, dropout_rate)\n",
    "                    model.train(groupsTr, weightsTr, groupsVal, weightsVal, batchSize)\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zmail\n",
    "def send_alert_email(subject, content_text, address):\n",
    "    user = 'send@tabchen.com'\n",
    "    passwd = 'Hello2Me.com'\n",
    "    server = zmail.server(\n",
    "    user,\n",
    "    passwd,\n",
    "    smtp_host='smtp.exmail.qq.com',\n",
    "    smtp_port=465,\n",
    "    smtp_ssl=True,\n",
    "    pop_host='pop.exmail.qq.com',\n",
    "    pop_port=995)\n",
    "    mail = {\n",
    "    'subject': subject,  # Anything you want.\n",
    "    'content_text': content_text,  # Anything you want.\n",
    "    }\n",
    "    return server.send_mail(address, mail)\n",
    "\n",
    "send_alert_email('Task-training RLL', 'jobs done!', 'emmitt@live.cn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
